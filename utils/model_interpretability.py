# -*- coding: utf-8 -*-
"""
æ¨¡å‹è§£é‡Šæ€§æ¨¡å—
æ”¹è¿›8ï¼šå®ç°SHAPå€¼è®¡ç®—å’Œæ³¨æ„åŠ›å¯è§†åŒ–

åŠŸèƒ½:
- ç‰¹å¾é‡è¦æ€§åˆ†æ
- SHAPå€¼è¿‘ä¼¼è®¡ç®—
- æ³¨æ„åŠ›æƒé‡æå–
- ç‰¹å¾è´¡çŒ®åº¦åˆ†æ
- é¢„æµ‹è§£é‡ŠæŠ¥å‘Š
"""

import os
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any, Union, Callable
import numpy as np
import pandas as pd
from collections import defaultdict

try:
    import torch
    import torch.nn as nn
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False
    torch = None
    nn = None

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from config.config import Config
from utils.logger import get_logger

logger = get_logger("ModelInterpretability")


class PermutationImportance:
    """æ’åˆ—ç‰¹å¾é‡è¦æ€§"""
    
    def __init__(self, model: 'nn.Module', 
                 loss_fn: Callable = None,
                 n_repeats: int = 10):
        """
        åˆå§‹åŒ–æ’åˆ—é‡è¦æ€§è®¡ç®—å™¨
        
        Args:
            model: PyTorchæ¨¡å‹
            loss_fn: æŸå¤±å‡½æ•°ï¼ˆç”¨äºè¯„ä¼°é¢„æµ‹è´¨é‡ï¼‰
            n_repeats: æ¯ä¸ªç‰¹å¾çš„é‡å¤æ¬¡æ•°
        """
        if not HAS_TORCH:
            raise ImportError("è¯·å®‰è£…PyTorch")
        
        self.model = model
        self.loss_fn = loss_fn or nn.MSELoss()
        self.n_repeats = n_repeats
    
    def calculate(self,
                  X: 'torch.Tensor',
                  y: 'torch.Tensor',
                  feature_names: List[str] = None) -> pd.DataFrame:
        """
        è®¡ç®—ç‰¹å¾é‡è¦æ€§
        
        é€šè¿‡æ‰“ä¹±æ¯ä¸ªç‰¹å¾å¹¶æµ‹é‡é¢„æµ‹è´¨é‡ä¸‹é™æ¥è¯„ä¼°é‡è¦æ€§
        
        Args:
            X: è¾“å…¥æ•°æ® (batch, seq_len, features) æˆ– (batch, features)
            y: çœŸå®æ ‡ç­¾
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            
        Returns:
            ç‰¹å¾é‡è¦æ€§DataFrame
        """
        self.model.eval()
        
        # è·å–åŸºå‡†æ€§èƒ½
        with torch.no_grad():
            base_pred = self.model(X)
            base_loss = self.loss_fn(base_pred, y).item()
        
        n_features = X.shape[-1]
        
        if feature_names is None:
            feature_names = [f'feature_{i}' for i in range(n_features)]
        
        importances = []
        
        for feat_idx in range(n_features):
            feat_losses = []
            
            for _ in range(self.n_repeats):
                # æ‰“ä¹±è¯¥ç‰¹å¾
                X_permuted = X.clone()
                perm_idx = torch.randperm(X.shape[0])
                
                if X.dim() == 3:  # (batch, seq, features)
                    X_permuted[:, :, feat_idx] = X[perm_idx, :, feat_idx]
                else:  # (batch, features)
                    X_permuted[:, feat_idx] = X[perm_idx, feat_idx]
                
                # è®¡ç®—æ‰“ä¹±åçš„æŸå¤±
                with torch.no_grad():
                    perm_pred = self.model(X_permuted)
                    perm_loss = self.loss_fn(perm_pred, y).item()
                
                feat_losses.append(perm_loss)
            
            # é‡è¦æ€§ = æ‰“ä¹±åæŸå¤± - åŸºå‡†æŸå¤±
            importance_mean = np.mean(feat_losses) - base_loss
            importance_std = np.std(feat_losses)
            
            importances.append({
                'feature': feature_names[feat_idx],
                'importance': importance_mean,
                'importance_std': importance_std,
                'base_loss': base_loss,
            })
        
        df = pd.DataFrame(importances)
        df = df.sort_values('importance', ascending=False).reset_index(drop=True)
        
        # å½’ä¸€åŒ–
        total_importance = df['importance'].sum()
        if total_importance > 0:
            df['importance_pct'] = df['importance'] / total_importance * 100
        else:
            df['importance_pct'] = 0
        
        return df


class GradientBasedSaliency:
    """åŸºäºæ¢¯åº¦çš„æ˜¾è‘—æ€§åˆ†æ"""
    
    def __init__(self, model: 'nn.Module'):
        """
        åˆå§‹åŒ–æ¢¯åº¦æ˜¾è‘—æ€§åˆ†æå™¨
        
        Args:
            model: PyTorchæ¨¡å‹
        """
        if not HAS_TORCH:
            raise ImportError("è¯·å®‰è£…PyTorch")
        
        self.model = model
    
    def calculate_saliency(self, X: 'torch.Tensor') -> np.ndarray:
        """
        è®¡ç®—è¾“å…¥æ¢¯åº¦ï¼ˆæ˜¾è‘—æ€§å›¾ï¼‰
        
        Args:
            X: è¾“å…¥å¼ é‡
            
        Returns:
            æ˜¾è‘—æ€§å›¾
        """
        self.model.eval()
        X.requires_grad = True
        
        output = self.model(X)
        
        # å¯¹è¾“å‡ºæ±‚å’Œå¹¶åå‘ä¼ æ’­
        if output.dim() > 1:
            output = output.sum(dim=1)
        output = output.sum()
        
        output.backward()
        
        # æ¢¯åº¦ç»å¯¹å€¼ä½œä¸ºæ˜¾è‘—æ€§
        saliency = X.grad.abs().detach().cpu().numpy()
        
        return saliency
    
    def calculate_integrated_gradients(self,
                                        X: 'torch.Tensor',
                                        baseline: 'torch.Tensor' = None,
                                        n_steps: int = 50) -> np.ndarray:
        """
        è®¡ç®—ç§¯åˆ†æ¢¯åº¦ï¼ˆæ›´å‡†ç¡®çš„å½’å› æ–¹æ³•ï¼‰
        
        Args:
            X: è¾“å…¥å¼ é‡
            baseline: åŸºçº¿è¾“å…¥ï¼ˆé»˜è®¤ä¸ºé›¶ï¼‰
            n_steps: ç§¯åˆ†æ­¥æ•°
            
        Returns:
            ç§¯åˆ†æ¢¯åº¦
        """
        if baseline is None:
            baseline = torch.zeros_like(X)
        
        # ç”Ÿæˆä»baselineåˆ°Xçš„è·¯å¾„
        scaled_inputs = []
        for step in range(n_steps + 1):
            alpha = step / n_steps
            scaled = baseline + alpha * (X - baseline)
            scaled_inputs.append(scaled)
        
        scaled_inputs = torch.cat(scaled_inputs, dim=0)
        scaled_inputs.requires_grad = True
        
        self.model.eval()
        outputs = self.model(scaled_inputs)
        
        if outputs.dim() > 1:
            outputs = outputs.sum(dim=1)
        outputs = outputs.sum()
        
        outputs.backward()
        
        grads = scaled_inputs.grad.view(n_steps + 1, *X.shape)
        
        # æ¢¯å½¢ç§¯åˆ†
        avg_grads = (grads[:-1] + grads[1:]) / 2
        integrated_grads = avg_grads.mean(dim=0)
        
        # ä¹˜ä»¥è¾“å…¥å·®å¼‚
        ig = (X - baseline) * integrated_grads
        
        return ig.detach().cpu().numpy()


class AttentionExtractor:
    """æ³¨æ„åŠ›æƒé‡æå–å™¨"""
    
    def __init__(self, model: 'nn.Module'):
        """
        åˆå§‹åŒ–æ³¨æ„åŠ›æå–å™¨
        
        Args:
            model: PyTorchæ¨¡å‹ï¼ˆéœ€åŒ…å«æ³¨æ„åŠ›å±‚ï¼‰
        """
        if not HAS_TORCH:
            raise ImportError("è¯·å®‰è£…PyTorch")
        
        self.model = model
        self.attention_weights = {}
        self._register_hooks()
    
    def _register_hooks(self):
        """æ³¨å†Œé’©å­ä»¥æ•è·æ³¨æ„åŠ›æƒé‡"""
        
        def get_attention_hook(name):
            def hook(module, input, output):
                if isinstance(output, tuple) and len(output) >= 2:
                    # MultiheadAttentionè¿”å›(output, attention_weights)
                    if output[1] is not None:
                        self.attention_weights[name] = output[1].detach().cpu()
                        return
                
                # å°è¯•ä»æ¨¡å—å±æ€§è·å–
                if hasattr(module, 'attention_weights'):
                    self.attention_weights[name] = module.attention_weights.detach().cpu()
            
            return hook
        
        for name, module in self.model.named_modules():
            if 'attention' in name.lower() or isinstance(module, nn.MultiheadAttention):
                module.register_forward_hook(get_attention_hook(name))
    
    def extract(self, X: 'torch.Tensor') -> Dict[str, np.ndarray]:
        """
        æå–æ³¨æ„åŠ›æƒé‡
        
        Args:
            X: è¾“å…¥å¼ é‡
            
        Returns:
            æ³¨æ„åŠ›æƒé‡å­—å…¸
        """
        self.attention_weights.clear()
        self.model.eval()
        
        with torch.no_grad():
            _ = self.model(X)
        
        # è½¬æ¢ä¸ºnumpy
        result = {k: v.numpy() for k, v in self.attention_weights.items()}
        
        return result
    
    def visualize_attention(self, 
                           attention: np.ndarray,
                           tokens: List[str] = None) -> str:
        """
        ç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ–çš„æ–‡æœ¬è¡¨ç¤º
        
        Args:
            attention: æ³¨æ„åŠ›æƒé‡ (seq_len, seq_len)
            tokens: æ ‡è®°åˆ—è¡¨
            
        Returns:
            æ–‡æœ¬è¡¨ç¤º
        """
        if attention.ndim > 2:
            attention = attention.mean(axis=0)  # å¹³å‡å¤šå¤´
        
        if attention.ndim > 2:
            attention = attention[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        
        seq_len = attention.shape[0]
        
        if tokens is None:
            tokens = [f't{i}' for i in range(seq_len)]
        
        lines = []
        lines.append("Attention Heatmap:")
        lines.append("-" * (seq_len * 8 + 5))
        
        # è¡¨å¤´
        header = "     " + " ".join([f"{t:>6}" for t in tokens[:10]])
        lines.append(header)
        
        # æ•°æ®è¡Œ
        for i in range(min(10, seq_len)):
            row = f"{tokens[i]:>4} " + " ".join([f"{attention[i,j]:>6.3f}" for j in range(min(10, seq_len))])
            lines.append(row)
        
        return "\n".join(lines)


class FeatureContribution:
    """ç‰¹å¾è´¡çŒ®åº¦åˆ†æ"""
    
    def __init__(self, model: 'nn.Module'):
        if not HAS_TORCH:
            raise ImportError("è¯·å®‰è£…PyTorch")
        self.model = model
    
    def analyze_single_prediction(self,
                                   X: 'torch.Tensor',
                                   feature_names: List[str] = None,
                                   n_samples: int = 100) -> Dict:
        """
        åˆ†æå•ä¸ªé¢„æµ‹çš„ç‰¹å¾è´¡çŒ®
        
        ä½¿ç”¨è¿‘ä¼¼SHAPï¼šé€šè¿‡å¯¹æ¯”æ·»åŠ /ç§»é™¤ç‰¹å¾çš„é¢„æµ‹å˜åŒ–
        
        Args:
            X: å•ä¸ªæ ·æœ¬è¾“å…¥
            feature_names: ç‰¹å¾åç§°
            n_samples: é‡‡æ ·æ¬¡æ•°
            
        Returns:
            è´¡çŒ®åº¦åˆ†æç»“æœ
        """
        self.model.eval()
        
        X = X.unsqueeze(0) if X.dim() == 1 else X
        n_features = X.shape[-1]
        
        if feature_names is None:
            feature_names = [f'f{i}' for i in range(n_features)]
        
        # è·å–å®Œæ•´é¢„æµ‹
        with torch.no_grad():
            full_pred = self.model(X).item()
        
        contributions = {}
        
        for feat_idx in range(n_features):
            # é€šè¿‡é®è”½è¯¥ç‰¹å¾ä¼°è®¡è´¡çŒ®
            masked_preds = []
            
            for _ in range(n_samples):
                X_masked = X.clone()
                # ç”¨éšæœºæŠ½æ ·æ›¿æ¢è¯¥ç‰¹å¾
                random_value = torch.randn_like(X_masked[..., feat_idx])
                X_masked[..., feat_idx] = random_value
                
                with torch.no_grad():
                    masked_pred = self.model(X_masked).item()
                masked_preds.append(masked_pred)
            
            # è´¡çŒ® = å®Œæ•´é¢„æµ‹ - é®è”½åå¹³å‡é¢„æµ‹
            contribution = full_pred - np.mean(masked_preds)
            
            contributions[feature_names[feat_idx]] = {
                'contribution': float(contribution),
                'masked_mean': float(np.mean(masked_preds)),
                'masked_std': float(np.std(masked_preds)),
            }
        
        # æ’åº
        sorted_contributions = sorted(
            contributions.items(), 
            key=lambda x: abs(x[1]['contribution']), 
            reverse=True
        )
        
        return {
            'prediction': full_pred,
            'contributions': dict(sorted_contributions),
            'top_positive': [(k, v['contribution']) for k, v in sorted_contributions if v['contribution'] > 0][:5],
            'top_negative': [(k, v['contribution']) for k, v in sorted_contributions if v['contribution'] < 0][:5],
        }


class PredictionExplainer:
    """é¢„æµ‹è§£é‡Šå™¨"""
    
    def __init__(self, model: 'nn.Module', feature_names: List[str] = None):
        """
        åˆå§‹åŒ–é¢„æµ‹è§£é‡Šå™¨
        
        Args:
            model: PyTorchæ¨¡å‹
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        """
        if not HAS_TORCH:
            raise ImportError("è¯·å®‰è£…PyTorch")
        
        self.model = model
        self.feature_names = feature_names
        
        self.permutation_importance = PermutationImportance(model)
        self.saliency = GradientBasedSaliency(model)
        self.contribution = FeatureContribution(model)
    
    def explain_prediction(self,
                          X: 'torch.Tensor',
                          sample_idx: int = 0) -> Dict:
        """
        è§£é‡Šå•ä¸ªé¢„æµ‹
        
        Args:
            X: è¾“å…¥æ•°æ®
            sample_idx: æ ·æœ¬ç´¢å¼•
            
        Returns:
            è§£é‡Šç»“æœ
        """
        sample = X[sample_idx:sample_idx+1] if X.dim() > 1 else X.unsqueeze(0)
        
        self.model.eval()
        
        # è·å–é¢„æµ‹
        with torch.no_grad():
            prediction = self.model(sample).item()
        
        # æ¢¯åº¦æ˜¾è‘—æ€§
        try:
            saliency = self.saliency.calculate_saliency(sample.clone())
            saliency_scores = saliency.flatten()
        except Exception as e:
            logger.warning(f"Saliency calculation failed: {e}")
            saliency_scores = None
        
        # ç‰¹å¾è´¡çŒ®
        try:
            contributions = self.contribution.analyze_single_prediction(
                sample.squeeze(0), self.feature_names
            )
        except Exception as e:
            logger.warning(f"Contribution analysis failed: {e}")
            contributions = {}
        
        return {
            'prediction': prediction,
            'saliency_scores': saliency_scores.tolist() if saliency_scores is not None else None,
            'contributions': contributions,
            'sample_shape': list(sample.shape),
        }
    
    def generate_report(self, X: 'torch.Tensor', y: 'torch.Tensor' = None) -> str:
        """
        ç”Ÿæˆè§£é‡ŠæŠ¥å‘Š
        
        Args:
            X: è¾“å…¥æ•°æ®
            y: æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ–‡æœ¬æŠ¥å‘Š
        """
        report = []
        report.append("=" * 60)
        report.append("             æ¨¡å‹é¢„æµ‹è§£é‡ŠæŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}")
        report.append(f"æ ·æœ¬æ•°: {X.shape[0]}")
        report.append(f"ç‰¹å¾æ•°: {X.shape[-1]}")
        report.append("")
        
        # ç‰¹å¾é‡è¦æ€§
        if y is not None:
            report.append("ğŸ“Š ç‰¹å¾é‡è¦æ€§æ’åï¼ˆåŸºäºæ’åˆ—é‡è¦æ€§ï¼‰:")
            report.append("-" * 40)
            
            try:
                importance_df = self.permutation_importance.calculate(X, y, self.feature_names)
                for i, row in importance_df.head(10).iterrows():
                    bar = "â–ˆ" * int(row['importance_pct'] / 5)
                    report.append(f"  {row['feature']:<20} {row['importance_pct']:>6.2f}% {bar}")
            except Exception as e:
                report.append(f"  è®¡ç®—å¤±è´¥: {e}")
        
        report.append("")
        
        # æ ·æœ¬è§£é‡Šç¤ºä¾‹
        report.append("ğŸ“ æ ·æœ¬é¢„æµ‹è§£é‡Šï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰:")
        report.append("-" * 40)
        
        try:
            explanation = self.explain_prediction(X, 0)
            report.append(f"  é¢„æµ‹å€¼: {explanation['prediction']:.4f}")
            
            if explanation.get('contributions'):
                report.append("  ä¸»è¦æ­£å‘è´¡çŒ®:")
                for feat, contrib in explanation['contributions'].get('top_positive', [])[:3]:
                    report.append(f"    + {feat}: {contrib:+.4f}")
                
                report.append("  ä¸»è¦è´Ÿå‘è´¡çŒ®:")
                for feat, contrib in explanation['contributions'].get('top_negative', [])[:3]:
                    report.append(f"    - {feat}: {contrib:+.4f}")
        except Exception as e:
            report.append(f"  è§£é‡Šå¤±è´¥: {e}")
        
        report.append("")
        report.append("=" * 60)
        
        return "\n".join(report)


if __name__ == "__main__":
    print("æ¨¡å‹è§£é‡Šæ€§æ¨¡å—æµ‹è¯•")
    print("="*50)
    
    if not HAS_TORCH:
        print("éœ€è¦å®‰è£…PyTorchæ‰èƒ½è¿è¡Œæµ‹è¯•")
    else:
        # åˆ›å»ºæµ‹è¯•æ¨¡å‹
        class SimpleModel(nn.Module):
            def __init__(self, n_features=10):
                super().__init__()
                self.fc1 = nn.Linear(n_features, 32)
                self.fc2 = nn.Linear(32, 1)
            
            def forward(self, x):
                x = torch.relu(self.fc1(x))
                return self.fc2(x)
        
        model = SimpleModel(n_features=10)
        feature_names = [f'feature_{i}' for i in range(10)]
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        X = torch.randn(50, 10)
        y = torch.randn(50, 1)
        
        # æµ‹è¯•æ’åˆ—é‡è¦æ€§
        print("\n1. æ’åˆ—ç‰¹å¾é‡è¦æ€§æµ‹è¯•")
        perm_imp = PermutationImportance(model, n_repeats=5)
        importance_df = perm_imp.calculate(X, y, feature_names)
        print(importance_df.head().to_string(index=False))
        
        # æµ‹è¯•æ¢¯åº¦æ˜¾è‘—æ€§
        print("\n2. æ¢¯åº¦æ˜¾è‘—æ€§æµ‹è¯•")
        saliency = GradientBasedSaliency(model)
        sal_map = saliency.calculate_saliency(X[:1].clone())
        print(f"  æ˜¾è‘—æ€§å›¾å½¢çŠ¶: {sal_map.shape}")
        print(f"  æœ€é‡è¦ç‰¹å¾: feature_{np.argmax(sal_map)}")
        
        # æµ‹è¯•ç‰¹å¾è´¡çŒ®
        print("\n3. ç‰¹å¾è´¡çŒ®åˆ†ææµ‹è¯•")
        contrib = FeatureContribution(model)
        result = contrib.analyze_single_prediction(X[0], feature_names, n_samples=20)
        print(f"  é¢„æµ‹å€¼: {result['prediction']:.4f}")
        print("  Topæ­£å‘è´¡çŒ®:", result['top_positive'][:3])
        
        # æµ‹è¯•é¢„æµ‹è§£é‡Šå™¨
        print("\n4. é¢„æµ‹è§£é‡ŠæŠ¥å‘Š")
        explainer = PredictionExplainer(model, feature_names)
        report = explainer.generate_report(X, y)
        print(report)
        
        print("\næ¨¡å‹è§£é‡Šæ€§æ¨¡å—æµ‹è¯•å®Œæˆ!")
